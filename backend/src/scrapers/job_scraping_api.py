# ðŸš€ PYTHON JOB SCRAPING API INTEGRATION
# Connects Python scraping engine with Node.js backend

import sys
import os
import json
import asyncio
from typing import Dict, List, Any
import logging

# Add the scrapers directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from legal_job_scraper import LegalJobScraper, ScrapingConfig, JobListing

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class JobScrapingAPI:
    """
    ðŸ”— JOB SCRAPING API BRIDGE
    Connects Python scraping engine with Node.js backend
    """
    
    def __init__(self):
        self.scraper = None
        self.config = ScrapingConfig(
            delay_min=1.0,
            delay_max=2.5,
            max_concurrent=3,
            timeout=30,
            retry_attempts=2,
            respect_robots=True,
            user_agent_rotation=True
        )

    async def scrape_jobs_for_api(self, search_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        ðŸŽ¯ SCRAPE JOBS FOR API CONSUMPTION
        Returns jobs in API-friendly format
        """
        try:
            async with LegalJobScraper(self.config) as scraper:
                # Comprehensive job scraping
                jobs = await scraper.scrape_jobs_comprehensive(search_params)
                
                # Convert to API format
                api_jobs = []
                for job in jobs:
                    api_job = {
                        'id': job.job_id,
                        'title': job.title,
                        'company': job.company,
                        'location': job.location,
                        'salary': job.salary,
                        'description': job.description,
                        'requirements': job.requirements,
                        'jobType': job.job_type,
                        'experienceLevel': job.experience_level,
                        'postedDate': job.posted_date,
                        'applicationUrl': job.application_url,
                        'source': job.source,
                        'scrapedAt': new Date().toISOString(),
                        'relevanceScore': 0.85,  # Will be calculated by AI
                        'compatibility': 'high',
                        'aiInsights': {
                            'skillMatch': 85,
                            'experienceMatch': 78,
                            'locationPreference': 92,
                            'salaryAlignment': 88
                        }
                    }
                    api_jobs.append(api_job)
                
                # Group by source
                sources = {}
                for job in api_jobs:
                    source = job['source']
                    if source not in sources:
                        sources[source] = []
                    sources[source].append(job)
                
                return {
                    'success': True,
                    'totalJobs': len(api_jobs),
                    'sources': list(sources.keys()),
                    'jobsBySource': {source: len(jobs) for source, jobs in sources.items()},
                    'jobs': api_jobs,
                    'scrapingMetadata': {
                        'scrapedAt': new Date().toISOString(),
                        'searchParams': search_params,
                        'legalCompliance': True,
                        'robotsTxtRespected': True,
                        'rateLimited': True
                    }
                }
                
        except Exception as e:
            logger.error(f"Error in job scraping API: {e}")
            return {
                'success': False,
                'error': str(e),
                'totalJobs': 0,
                'jobs': []
            }

def main():
    """
    ðŸŽ¯ MAIN API FUNCTION
    Called by Node.js backend
    """
    # Get search parameters from command line arguments
    if len(sys.argv) > 1:
        try:
            search_params = json.loads(sys.argv[1])
        except json.JSONDecodeError:
            search_params = {
                'keywords': 'software developer',
                'location': 'remote',
                'experience_level': 'mid',
                'job_type': 'full-time'
            }
    else:
        search_params = {
            'keywords': 'software developer',
            'location': 'remote',
            'experience_level': 'mid',
            'job_type': 'full-time'
        }
    
    # Create API instance and scrape jobs
    api = JobScrapingAPI()
    
    async def run_scraping():
        result = await api.scrape_jobs_for_api(search_params)
        print(json.dumps(result, indent=2))
    
    # Run the async function
    asyncio.run(run_scraping())

if __name__ == "__main__":
    main()